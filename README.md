
# Text generation project

  This project investigates advancements in text generation models, comparing vanilla RNNs, LSTMs, and Transformers. Initially, we implemented and evaluated a vanilla RNN and one-layer and two-layer LSTM networks using character-level inputs. Subsequently, we explored tokenization techniques like Word2vec and Byte-Pair Encoding (BPE) to enhance LSTM performance. Finally, we implemented a decoder-only Transformer model. Our results indicate that while LSTMs outperform vanilla RNNs in capturing long-term dependencies, the Transformer model achieves superior performance, particularly in reducing misspelling rates. The best model utilized BPE with a vocabulary size of 5000 demonstrated improvements in text generation tasks.


