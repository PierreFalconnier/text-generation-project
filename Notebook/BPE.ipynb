{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-Pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import sys\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from Run.misspelling_percentage import calculate_misspelling_percentage\n",
    "\n",
    "\n",
    "# Import custom dataset and model\n",
    "from Dataset.DatasetText import DatasetText as Dataset\n",
    "from Models.RNN import RNN\n",
    "from Models import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to the MInbpe repository\n",
    "MINBPE_PATH = Path.cwd().parent.parent / 'MInbpe'\n",
    "if str(MINBPE_PATH) not in sys.path:\n",
    "    sys.path.append(str(MINBPE_PATH))\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "\n",
    "# Add the root to the system path\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minbpe\n",
    "from minbpe import RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size before BPE 101\n",
      "Initial vocabulary before BPE [' ', 'e', 't', 'a', 'o', 'n', 'r', 'i', 'h', 's', 'd', 'l', 'u', 'g', 'y', 'w', 'm', 'c', 'f', '.', ',', 'p', '\"', '\\n', 'b', 'k', 'v', 'H', \"'\", 'I', '-', 'T', 'S', 'M', '?', 'W', 'D', 'R', 'A', 'P', '!', 'x', 'B', 'G', 'C', 'N', 'j', 'Y', 'L', 'F', 'z', 'O', 'E', ';', 'q', '\\xad', 'V', 'U', 'K', ':', 'Q', 'J', '1', '*', ')', '(', '2', '3', '4', '0', 'X', '5', '9', '6', 'Z', '8', '7', '_', '`', '/', '=', '\\x93', 'é', '\\\\', '%', '$', ']', '\\x95', '\\x96', '&', '¦', '~', '^', '«', 'ù', '}', '{', '[', '»', '>', '\\x1f']\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"harry_potter.txt\"\n",
    "mode = \"character\"\n",
    "sequence_length = 100\n",
    "\n",
    "# Load the dataset (without BPE)\n",
    "folder_path = ROOT / \"Data\"  / dataset_name\n",
    "dataset = Dataset(folder_path=folder_path, sequence_length=sequence_length, mode=mode)\n",
    "print(\"Vocabulary size before BPE\",dataset.vocab_size)\n",
    "print(\"Initial vocabulary before BPE\", dataset.uniq_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "with open(folder_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokenizer.train(text, vocab_size=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.encode(\"hello world\") # string -> tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.decode([1000, 2000, 3000]) # tokens -> string\n",
    "tokenizer.save(\"tok32k\") # writes tok32k.model and tok32k.vocab\n",
    "tokenizer.load(\"tok32k.model\") # loads the model back from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "tokenizer.train(, vocab_size=32768)\n",
    "tokenizer.encode(\"hello world\") # string -> tokens\n",
    "tokenizer.decode([1000, 2000, 3000]) # tokens -> string\n",
    "tokenizer.save(\"tok32k\") # writes tok32k.model and tok32k.vocab\n",
    "tokenizer.load(\"tok32k.model\") # loads the model back from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM training (with BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 220\u001b[0m\n\u001b[0;32m    218\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Set this to a higher number for better results\u001b[39;00m\n\u001b[0;32m    219\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m--> 220\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Step 8: Generate text using the trained model\u001b[39;00m\n\u001b[0;32m    223\u001b[0m init_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHarry\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[36], line 208\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m    205\u001b[0m state_h, state_c \u001b[38;5;241m=\u001b[39m state_h\u001b[38;5;241m.\u001b[39mto(device), state_c\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    207\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 208\u001b[0m y_pred, (state_h, state_c) \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_c\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m), y)\n\u001b[0;32m    210\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\karim\\.conda\\envs\\deep_learning_updated\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\karim\\.conda\\envs\\deep_learning_updated\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[36], line 106\u001b[0m, in \u001b[0;36mLSTM_BPE.forward\u001b[1;34m(self, x, states)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, states):\n\u001b[0;32m    105\u001b[0m     embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m--> 106\u001b[0m     output, states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, states\n",
      "File \u001b[1;32mc:\\Users\\karim\\.conda\\envs\\deep_learning_updated\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\karim\\.conda\\envs\\deep_learning_updated\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\karim\\.conda\\envs\\deep_learning_updated\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:903\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    901\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    902\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 2-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 903\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    904\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from minbpe import RegexTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class DatasetTextBPE(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path, sequence_length, mode=\"word\", use_bpe=False, vocab_size=300):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.folder_path = folder_path\n",
    "        self.mode = mode\n",
    "        self.use_bpe = use_bpe\n",
    "\n",
    "        # Load and process text data\n",
    "        self.words = self.load_words()\n",
    "\n",
    "        if self.use_bpe:\n",
    "            self.tokenizer = RegexTokenizer()\n",
    "            self.train_tokenizer(self.words, vocab_size)\n",
    "            self.words_indexes = self.encode_words(self.words)\n",
    "            self.vocab_size = len(self.tokenizer.vocab)\n",
    "        else:\n",
    "            self.uniq_words = self.get_uniq_words()\n",
    "            self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "            self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "            self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "            self.vocab_size = len(self.uniq_words)\n",
    "\n",
    "    def load_words(self):\n",
    "        with open(self.folder_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "        if self.mode == \"word\":\n",
    "            return text.split()\n",
    "        elif self.mode == \"character\":\n",
    "            return list(text)\n",
    "        else:\n",
    "            raise ValueError(\"wrong mode\")\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def train_tokenizer(self, text, vocab_size):\n",
    "        # Join the text into a single string if mode is word, else keep as is\n",
    "        very_long_training_string = ' '.join(text) if self.mode == \"word\" else ''.join(text)\n",
    "        self.tokenizer.train(very_long_training_string, vocab_size)\n",
    "        self.tokenizer.save(\"tok32k\")\n",
    "\n",
    "    def encode_words(self, text):\n",
    "        very_long_training_string = ' '.join(text) if self.mode == \"word\" else ''.join(text)\n",
    "        return self.tokenizer.encode(very_long_training_string)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) // (self.sequence_length + 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.sequence_length\n",
    "        end_index = start_index + self.sequence_length\n",
    "\n",
    "        input_indices = self.words_indexes[start_index:end_index]\n",
    "        target_indices = self.words_indexes[start_index + 1 : end_index + 1]\n",
    "\n",
    "        return torch.tensor(input_indices), torch.tensor(target_indices)\n",
    "\n",
    "class LSTM_BPE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        hidden_dim=100,\n",
    "        embedding_dim=None,\n",
    "        num_layers=1,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super(LSTM_BPE, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if embedding_dim is not None:\n",
    "            # Learned embedding\n",
    "            self.embedding = nn.Embedding(\n",
    "                num_embeddings=vocab_size,\n",
    "                embedding_dim=self.embedding_dim,\n",
    "            )\n",
    "            input_size = self.embedding_dim\n",
    "        else:\n",
    "            # Assume input is already one-hot encoded\n",
    "            input_size = vocab_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        embed = self.embedding(x) if self.embedding_dim is not None else x\n",
    "        output, states = self.lstm(embed, states)\n",
    "        logits = self.fc(output)\n",
    "        return logits, states\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        # init hidden and cell states\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "        )\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        dataset,\n",
    "        device,\n",
    "        text,\n",
    "        total_length=1000,\n",
    "        temperature=1.0,\n",
    "        mode=\"character\",\n",
    "        top_p=0.9,\n",
    "        nucleus_sampling=False,\n",
    "    ):\n",
    "        self.eval()\n",
    "\n",
    "        if mode == \"word\":\n",
    "            words = text.split(\" \")\n",
    "        elif mode == \"character\":\n",
    "            words = list(text)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        state_h, state_c = self.init_state(1)\n",
    "        state_h, state_c = state_h.to(device), state_c.to(device)\n",
    "\n",
    "        for i in range(total_length):\n",
    "            # Map input text to token indices using the dataset's token mappings\n",
    "            if dataset.use_bpe:\n",
    "                x = torch.tensor([[dataset.token_to_index[token] for token in dataset.tokenizer.encode(text[i:])]]).to(device)\n",
    "            else:\n",
    "                x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
    "\n",
    "            if self.embedding_dim is None:\n",
    "                x = F.one_hot(x, num_classes=self.vocab_size).float()\n",
    "\n",
    "            y_pred, (state_h, state_c) = self(x, (state_h, state_c))\n",
    "\n",
    "            last_word_logits = y_pred[0][-1] / temperature\n",
    "            probs = torch.nn.functional.softmax(last_word_logits, dim=0)\n",
    "\n",
    "            if nucleus_sampling:\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_probs[sorted_indices_to_remove] = 0.0\n",
    "                sorted_probs /= sorted_probs.sum()  # normalize\n",
    "                word_index = torch.multinomial(sorted_probs, 1).item()\n",
    "            else:\n",
    "                word_index = torch.multinomial(probs, 1).item()\n",
    "\n",
    "            if dataset.use_bpe:\n",
    "                words.append(dataset.index_to_token[word_index])\n",
    "            else:\n",
    "                words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "        return words\n",
    "\n",
    "# Step 3: Initialize the dataset\n",
    "folder_path = os.path.join(\"..\", \"Data\", \"harry_potter.txt\")  \n",
    "sequence_length = 100\n",
    "use_bpe = True\n",
    "vocab_size = 256\n",
    "\n",
    "dataset = DatasetTextBPE(folder_path=folder_path, sequence_length=sequence_length, use_bpe=use_bpe, vocab_size=vocab_size)\n",
    "\n",
    "# Step 4: Create data loader\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Step 5: Initialize the model\n",
    "hidden_dim = 1024\n",
    "embedding_dim = None\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTM_BPE(vocab_size=dataset.vocab_size, hidden_dim=hidden_dim, embedding_dim=embedding_dim, num_layers=num_layers, dropout=dropout).to(device)\n",
    "\n",
    "# Step 6: Define the training loop\n",
    "def train_model(model, dataloader, num_epochs, learning_rate):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            state_h, state_c = model.init_state(x.size(0))\n",
    "            state_h, state_c = state_h.to(device), state_c.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.permute(0, 2, 1), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Step 7: Train the model\n",
    "num_epochs = 10  # Set this to a higher number for better results\n",
    "learning_rate = 0.001\n",
    "train_model(model, train_dataloader, num_epochs, learning_rate)\n",
    "\n",
    "# Step 8: Generate text using the trained model\n",
    "init_text = \"Harry\"\n",
    "generated_text = model.generate(dataset, device, text=init_text, total_length=100, temperature=1.0, mode=\"character\", top_p=0.9, nucleus_sampling=True)\n",
    "print(\"Generated text:\", \"\".join(generated_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
